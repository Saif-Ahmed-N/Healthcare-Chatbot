# backend/rag_integration.py
import os
from langchain_community.llms import Ollama
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings # Updated import
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from .llm_integration import get_llm, query_llm # Import query_llm for fallback
from typing import Dict, Any, List, Optional, Text

# --- Global RAG Pipeline ---
rag_qa_chain: Optional[RetrievalQA] = None

def get_mock_policy_documents():
    """
    Expanded knowledge base.
    """
    return [
        ("ACCEPTED INSURANCE PLANS: We accept Blue Cross Blue Shield (Gold, Silver, Platinum), Aetna (HMO/PPO), Cigna, and Medicare Part B. We DO NOT accept Medicaid.", {"source": "Insurance Policy"}),
        ("CO-PAYS: GP visits are $25. Specialist visits are $50. Out-of-network requires 50% upfront payment.", {"source": "Billing Guide"}),
        ("CANCELLATION POLICY: 24-hour notice required. Late cancellations incur a $25 fee. No-shows are charged $50.", {"source": "Appointment Rules"}),
        ("PHARMACY: Our in-house pharmacy is open 9 AM - 6 PM. We accept e-prescriptions directly from doctors.", {"source": "Pharmacy Guide"}),
        ("LAB TESTS: Blood tests require fasting for 8 hours. Results are available in 24-48 hours via the patient portal.", {"source": "Lab Guide"}),
    ]

def initialize_rag_pipeline():
    global rag_qa_chain
    if rag_qa_chain: return

    print("RAG: Initializing Hybrid RAG pipeline...")
    docs_data = get_mock_policy_documents()
    texts = [d[0] for d in docs_data]
    metadatas = [d[1] for d in docs_data]
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    splits = text_splitter.create_documents(texts, metadatas=metadatas)
    
    # Use the updated class to avoid warnings
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    faiss_index = FAISS.from_documents(splits, embeddings)
    retriever = faiss_index.as_retriever(search_kwargs={"k": 2}) # Strict context
    
    llm = get_llm()
    rag_qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    print("RAG: Pipeline initialized.")

async def query_rag(query: str) -> Dict[Text, Any]:
    global rag_qa_chain
    if not rag_qa_chain:
        return {"answer": "System initializing, please try again.", "sources": []}

    # 1. Try to answer with RAG (Hospital Policy)
    try:
        # We verify if the retriever actually finds relevant docs
        docs = rag_qa_chain.retriever.get_relevant_documents(query)
        
        if not docs:
            # 2. FALLBACK: No internal docs found? Ask the LLM generally.
            print(f"RAG: No specific policy found for '{query}'. Falling back to LLM.")
            llm_answer = await query_llm(query)
            return {
                "answer": f"I couldn't find a specific hospital policy for that, but generally speaking: {llm_answer}\n\n*(Generated by AI)*",
                "sources": ["General Medical Knowledge"]
            }

        # If docs found, generate specific answer
        result = rag_qa_chain.invoke(query)
        answer = result.get("result", "").strip()
        
        # Check for hallucination or refusal
        if "I cannot find" in answer or "I don't know" in answer:
             llm_answer = await query_llm(query)
             return {
                "answer": f"Our policy documents don't cover that explicitly. Generally: {llm_answer}",
                "sources": ["General Medical Knowledge"]
            }

        sources = list(set([doc.metadata.get("source", "Unknown") for doc in result["source_documents"]]))
        return {"answer": answer, "sources": sources}

    except Exception as e:
        print(f"RAG Error: {e}")
        return {"answer": "Sorry, I'm having trouble accessing the knowledge base.", "sources": []}